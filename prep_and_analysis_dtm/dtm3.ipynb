{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1660f1d0",
   "metadata": {},
   "source": [
    "All necessary information for setting up dtm: \n",
    "https://github.com/blei-lab/dtm/blob/master/README.md <- Compile binaries manually from Blei C++ \n",
    "https://github.com/piskvorky/gensim/blob/release-3.8.3/gensim/models/wrappers/dtmmodel.py As the gensim wrapper for Blei's DTM is not supported anymore: Compile dtm-code myself first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac51bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "import os\n",
    "from subprocess import PIPE\n",
    "import numpy as np\n",
    "\n",
    "from gensim import utils, corpora, matutils\n",
    "from gensim.utils import check_output\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DtmModel(utils.SaveLoad):\n",
    "    \"\"\"Python wrapper using `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_.\n",
    "\n",
    "    Communication between DTM and Python takes place by passing around data files on disk and executing\n",
    "    the DTM binary as a subprocess.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    This is **only** python wrapper for `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_,\n",
    "    you need to install original implementation first and pass the path to binary to ``dtm_path``.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dtm_path, corpus=None, time_slices=None, mode='fit', model='dtm', num_topics=100,\n",
    "                 id2word=None, prefix=None, lda_sequence_min_iter=6, lda_sequence_max_iter=20, lda_max_em_iter=10,\n",
    "                 alpha=0.01, top_chain_var=0.005, rng_seed=0, initialize_lda=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dtm_path : str\n",
    "            Path to the dtm binary, e.g. `/home/username/dtm/dtm/main`.\n",
    "        corpus : iterable of iterable of (int, int)\n",
    "            Collection of texts in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "        mode : {'fit', 'time'}, optional\n",
    "            Controls the mode of the mode: 'fit' is for training, 'time' for analyzing documents through time\n",
    "            according to a DTM, basically a held out set.\n",
    "        model : {'fixed', 'dtm'}, optional\n",
    "            Control model that will be runned: 'fixed' is for DIM and 'dtm' for DTM.\n",
    "        num_topics : int, optional\n",
    "            Number of topics.\n",
    "        id2word : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
    "            Mapping between tokens ids and words from corpus, if not specified - will be inferred from `corpus`.\n",
    "        prefix : str, optional\n",
    "            Prefix for produced temporary files.\n",
    "        lda_sequence_min_iter : int, optional\n",
    "             Min iteration of LDA.\n",
    "        lda_sequence_max_iter : int, optional\n",
    "            Max iteration of LDA.\n",
    "        lda_max_em_iter : int, optional\n",
    "             Max em optimization iterations in LDA.\n",
    "        alpha : int, optional\n",
    "            Hyperparameter that affects sparsity of the document-topics for the LDA models in each timeslice.\n",
    "        top_chain_var : float, optional\n",
    "            This hyperparameter controls one of the key aspect of topic evolution which is the speed at which\n",
    "            these topics evolve. A smaller top_chain_var leads to similar word distributions over multiple timeslice.\n",
    "\n",
    "        rng_seed : int, optional\n",
    "             Random seed.\n",
    "        initialize_lda : bool, optional\n",
    "             If True - initialize DTM with LDA.\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(dtm_path):\n",
    "            raise ValueError(\"dtm_path must point to the binary file, not to a folder\")\n",
    "\n",
    "        self.dtm_path = dtm_path\n",
    "        self.id2word = id2word\n",
    "        if self.id2word is None:\n",
    "            logger.warning(\"no word id mapping provided; initializing from corpus, assuming identity\")\n",
    "            self.id2word = utils.dict_from_corpus(corpus)\n",
    "            self.num_terms = len(self.id2word)\n",
    "        else:\n",
    "            self.num_terms = 0 if not self.id2word else 1 + max(self.id2word.keys())\n",
    "        if self.num_terms == 0:\n",
    "            raise ValueError(\"cannot compute DTM over an empty collection (no terms)\")\n",
    "        self.num_topics = num_topics\n",
    "\n",
    "        try:\n",
    "            lencorpus = len(corpus)\n",
    "        except TypeError:\n",
    "            logger.warning(\"input corpus stream has no len(); counting documents\")\n",
    "            lencorpus = sum(1 for _ in corpus)\n",
    "        if lencorpus == 0:\n",
    "            raise ValueError(\"cannot compute DTM over an empty corpus\")\n",
    "        if model == \"fixed\" and any(not text for text in corpus):\n",
    "            raise ValueError(\"\"\"There is a text without words in the input corpus.\n",
    "                    This breaks method='fixed' (The DIM model).\"\"\")\n",
    "        if lencorpus != sum(time_slices):\n",
    "            raise ValueError(\n",
    "                \"mismatched timeslices %{slices} for corpus of len {clen}\"\n",
    "                .format(slices=sum(time_slices), clen=lencorpus)\n",
    "            )\n",
    "        self.lencorpus = lencorpus\n",
    "        if prefix is None:\n",
    "            rand_prefix = hex(random.randint(0, 0xffffff))[2:] + '_'\n",
    "            prefix = os.path.join(tempfile.gettempdir(), rand_prefix)\n",
    "\n",
    "        self.prefix = prefix\n",
    "        self.time_slices = time_slices\n",
    "        self.lda_sequence_min_iter = int(lda_sequence_min_iter)\n",
    "        self.lda_sequence_max_iter = int(lda_sequence_max_iter)\n",
    "        self.lda_max_em_iter = int(lda_max_em_iter)\n",
    "        self.alpha = alpha\n",
    "        self.top_chain_var = top_chain_var\n",
    "        self.rng_seed = rng_seed\n",
    "        self.initialize_lda = str(initialize_lda).lower()\n",
    "\n",
    "        self.lambda_ = None\n",
    "        self.obs_ = None\n",
    "        self.lhood_ = None\n",
    "        self.gamma_ = None\n",
    "        self.init_alpha = None\n",
    "        self.init_beta = None\n",
    "        self.init_ss = None\n",
    "        self.em_steps = []\n",
    "        self.influences_time = []\n",
    "\n",
    "        if corpus is not None:\n",
    "            self.train(corpus, time_slices, mode, model)\n",
    "\n",
    "    def fout_liklihoods(self):\n",
    "        \"\"\"Get path to temporary lhood data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to lhood data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'lhoods.dat'\n",
    "\n",
    "    def fout_gamma(self):\n",
    "        \"\"\"Get path to temporary gamma data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to gamma data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'gam.dat'\n",
    "\n",
    "    def fout_prob(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'topic-{i}-var-e-log-prob.dat'\n",
    "\n",
    "    def fout_observations(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'topic-{i}-var-obs.dat'\n",
    "\n",
    "    def fout_influence(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'influence_time-{i}'\n",
    "\n",
    "    def foutname(self):\n",
    "        \"\"\"Get path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out'\n",
    "\n",
    "    def fem_steps(self):\n",
    "        \"\"\"Get path to temporary em_step data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to em_step data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'em_log.dat'\n",
    "\n",
    "    def finit_alpha(self):\n",
    "        \"\"\"Get path to initially trained lda alpha file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initially trained lda alpha file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda.alpha'\n",
    "\n",
    "    def finit_beta(self):\n",
    "        \"\"\"Get path to initially trained lda beta file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initially trained lda beta file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda.beta'\n",
    "\n",
    "    def flda_ss(self):\n",
    "        \"\"\"Get path to initial lda binary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initial lda binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda-ss.dat'\n",
    "\n",
    "    def fcorpustxt(self):\n",
    "        \"\"\"Get path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to multiple train binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train-mult.dat'\n",
    "\n",
    "    def fcorpus(self):\n",
    "        \"\"\"Get path to corpus file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to corpus file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train'\n",
    "\n",
    "    def ftimeslices(self):\n",
    "        \"\"\"Get path to time slices binary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to time slices binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train-seq.dat'\n",
    "\n",
    "    def convert_input(self, corpus, time_slices):\n",
    "        \"\"\"Convert corpus into LDA-C format by :class:`~gensim.corpora.bleicorpus.BleiCorpus` and save to temp file.\n",
    "        Path to temporary file produced by :meth:`~gensim.models.wrappers.dtmmodel.DtmModel.ftimeslices`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, float)\n",
    "            Corpus in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"serializing temporary corpus to %s\", self.fcorpustxt())\n",
    "        # write out the corpus in a file format that DTM understands:\n",
    "        corpora.BleiCorpus.save_corpus(self.fcorpustxt(), corpus)\n",
    "\n",
    "        with utils.open(self.ftimeslices(), 'wb') as fout:\n",
    "            fout.write(utils.to_utf8(str(len(self.time_slices)) + \"\\n\"))\n",
    "            for sl in time_slices:\n",
    "                fout.write(utils.to_utf8(str(sl) + \"\\n\"))\n",
    "\n",
    "    def train(self, corpus, time_slices, mode, model):\n",
    "        \"\"\"Train DTM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, int)\n",
    "            Collection of texts in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "        mode : {'fit', 'time'}, optional\n",
    "            Controls the mode of the mode: 'fit' is for training, 'time' for analyzing documents through time\n",
    "            according to a DTM, basically a held out set.\n",
    "        model : {'fixed', 'dtm'}, optional\n",
    "            Control model that will be runned: 'fixed' is for DIM and 'dtm' for DTM.\n",
    "\n",
    "        \"\"\"\n",
    "        self.convert_input(corpus, time_slices)\n",
    "\n",
    "        arguments = \\\n",
    "            \"--ntopics={p0} --model={mofrl}  --mode={p1} --initialize_lda={p2} --corpus_prefix={p3} \" \\\n",
    "            \"--outname={p4} --alpha={p5}\".format(\n",
    "                p0=self.num_topics, mofrl=model, p1=mode, p2=self.initialize_lda,\n",
    "                p3=self.fcorpus(), p4=self.foutname(), p5=self.alpha\n",
    "            )\n",
    "\n",
    "        params = \\\n",
    "            \"--lda_max_em_iter={p0} --lda_sequence_min_iter={p1}  --lda_sequence_max_iter={p2} \" \\\n",
    "            \"--top_chain_var={p3} --rng_seed={p4} \".format(\n",
    "                p0=self.lda_max_em_iter, p1=self.lda_sequence_min_iter, p2=self.lda_sequence_max_iter,\n",
    "                p3=self.top_chain_var, p4=self.rng_seed\n",
    "            )\n",
    "\n",
    "        arguments = arguments + \" \" + params\n",
    "        logger.info(\"training DTM with args %s\", arguments)\n",
    "\n",
    "        cmd = [self.dtm_path] + arguments.split()\n",
    "        logger.info(\"Running command %s\", cmd)\n",
    "        check_output(args=cmd, stderr=PIPE)\n",
    "\n",
    "        self.em_steps = np.loadtxt(self.fem_steps())\n",
    "        self.init_ss = np.loadtxt(self.flda_ss())\n",
    "\n",
    "        if self.initialize_lda:\n",
    "            self.init_alpha = np.loadtxt(self.finit_alpha())\n",
    "            self.init_beta = np.loadtxt(self.finit_beta())\n",
    "\n",
    "        self.lhood_ = np.loadtxt(self.fout_liklihoods())\n",
    "\n",
    "        # document-topic proportions\n",
    "        self.gamma_ = np.loadtxt(self.fout_gamma())\n",
    "        # cast to correct shape, gamme[5,10] is the proprtion of the 10th topic\n",
    "        # in doc 5\n",
    "        self.gamma_.shape = (self.lencorpus, self.num_topics)\n",
    "        # normalize proportions\n",
    "        self.gamma_ /= self.gamma_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        self.lambda_ = np.zeros((self.num_topics, self.num_terms * len(self.time_slices)))\n",
    "        self.obs_ = np.zeros((self.num_topics, self.num_terms * len(self.time_slices)))\n",
    "\n",
    "        for t in range(self.num_topics):\n",
    "            topic = \"%03d\" % t\n",
    "            self.lambda_[t, :] = np.loadtxt(self.fout_prob().format(i=topic))\n",
    "            self.obs_[t, :] = np.loadtxt(self.fout_observations().format(i=topic))\n",
    "        # cast to correct shape, lambda[5,10,0] is the proportion of the 10th\n",
    "        # topic in doc 5 at time 0\n",
    "        self.lambda_.shape = (self.num_topics, self.num_terms, len(self.time_slices))\n",
    "        self.obs_.shape = (self.num_topics, self.num_terms, len(self.time_slices))\n",
    "        # extract document influence on topics for each time slice\n",
    "        # influences_time[0] , influences at time 0\n",
    "        if model == 'fixed':\n",
    "            for k, t in enumerate(self.time_slices):\n",
    "                stamp = \"%03d\" % k\n",
    "                influence = np.loadtxt(self.fout_influence().format(i=stamp))\n",
    "                influence.shape = (t, self.num_topics)\n",
    "                # influence[2,5] influence of document 2 on topic 5\n",
    "                self.influences_time.append(influence)\n",
    "\n",
    "    def print_topics(self, num_topics=10, times=5, num_words=10):\n",
    "        \"\"\"Alias for :meth:`~gensim.models.wrappers.dtmmodel.DtmModel.show_topics`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics : int, optional\n",
    "            Number of topics to return, set `-1` to get all topics.\n",
    "        times : int, optional\n",
    "            Number of times.\n",
    "        num_words : int, optional\n",
    "            Number of words.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            Topics as a list of strings\n",
    "\n",
    "        \"\"\"\n",
    "        return self.show_topics(num_topics, times, num_words, log=True)\n",
    "\n",
    "    def show_topics(self, num_topics=10, times=5, num_words=10, log=False, formatted=True):\n",
    "        \"\"\"Get the `num_words` most probable words for `num_topics` number of topics at 'times' time slices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics : int, optional\n",
    "            Number of topics to return, set `-1` to get all topics.\n",
    "        times : int, optional\n",
    "            Number of times.\n",
    "        num_words : int, optional\n",
    "            Number of words.\n",
    "        log : bool, optional\n",
    "            THIS PARAMETER WILL BE IGNORED.\n",
    "        formatted : bool, optional\n",
    "            If `True` - return the topics as a list of strings, otherwise as lists of (weight, word) pairs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            Topics as a list of strings (if formatted=True) **OR**\n",
    "        list of (float, str)\n",
    "            Topics as list of (weight, word) pairs (if formatted=False)\n",
    "\n",
    "        \"\"\"\n",
    "        if num_topics < 0 or num_topics >= self.num_topics:\n",
    "            num_topics = self.num_topics\n",
    "            chosen_topics = range(num_topics)\n",
    "        else:\n",
    "            num_topics = min(num_topics, self.num_topics)\n",
    "            chosen_topics = range(num_topics)\n",
    "\n",
    "        if times < 0 or times >= len(self.time_slices):\n",
    "            times = len(self.time_slices)\n",
    "            chosen_times = range(times)\n",
    "        else:\n",
    "            times = min(times, len(self.time_slices))\n",
    "            chosen_times = range(times)\n",
    "\n",
    "        shown = []\n",
    "        for time in chosen_times:\n",
    "            for i in chosen_topics:\n",
    "                if formatted:\n",
    "                    topic = self.print_topic(i, time, topn=num_words)\n",
    "                else:\n",
    "                    topic = self.show_topic(i, time, topn=num_words)\n",
    "                shown.append(topic)\n",
    "        return shown\n",
    "\n",
    "    def show_topic(self, topicid, time, topn=50, num_words=None):\n",
    "        \"\"\"Get `num_words` most probable words for the given `topicid`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        topicid : int\n",
    "            Id of topic.\n",
    "        time : int\n",
    "            Timestamp.\n",
    "        topn : int, optional\n",
    "            Top number of topics that you'll receive.\n",
    "        num_words : int, optional\n",
    "            DEPRECATED PARAMETER, use `topn` instead.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of (float, str)\n",
    "            Sequence of probable words, as a list of `(word_probability, word)`.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_words is not None:  # deprecated num_words is used\n",
    "            warnings.warn(\"The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\")\n",
    "            topn = num_words\n",
    "\n",
    "        topics = self.lambda_[:, :, time]\n",
    "        topic = topics[topicid]\n",
    "        # likelihood to probability\n",
    "        topic = np.exp(topic)\n",
    "        # normalize to probability dist\n",
    "        topic = topic / topic.sum()\n",
    "        # sort according to prob\n",
    "        bestn = matutils.argsort(topic, topn, reverse=True)\n",
    "        beststr = [(topic[idx], self.id2word[idx]) for idx in bestn]\n",
    "        return beststr\n",
    "\n",
    "    def print_topic(self, topicid, time, topn=10, num_words=None):\n",
    "        \"\"\"Get the given topic, formatted as a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        topicid : int\n",
    "            Id of topic.\n",
    "        time : int\n",
    "            Timestamp.\n",
    "        topn : int, optional\n",
    "            Top number of topics that you'll receive.\n",
    "        num_words : int, optional\n",
    "            DEPRECATED PARAMETER, use `topn` instead.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The given topic in string format, like '0.132*someword + 0.412*otherword + ...'.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_words is not None:  # deprecated num_words is used\n",
    "            warnings.warn(\"The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\")\n",
    "            topn = num_words\n",
    "\n",
    "        return ' + '.join('%.3f*%s' % v for v in self.show_topic(topicid, time, topn=topn))\n",
    "\n",
    "    def dtm_vis(self, corpus, time):\n",
    "        \"\"\"Get data specified by pyLDAvis format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, float)\n",
    "            Collection of texts in BoW format.\n",
    "        time : int\n",
    "            Sequence of timestamp.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        All of these are needed to visualise topics for DTM for a particular time-slice via pyLDAvis.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        doc_topic : numpy.ndarray\n",
    "            Document-topic proportions.\n",
    "        topic_term : numpy.ndarray\n",
    "            Calculated term of topic suitable for pyLDAvis format.\n",
    "        doc_lengths : list of int\n",
    "            Length of each documents in corpus.\n",
    "        term_frequency : numpy.ndarray\n",
    "            Frequency of each word from vocab.\n",
    "        vocab : list of str\n",
    "            List of words from docpus.\n",
    "\n",
    "        \"\"\"\n",
    "        topic_term = np.exp(self.lambda_[:, :, time]) / np.exp(self.lambda_[:, :, time]).sum()\n",
    "        topic_term *= self.num_topics\n",
    "\n",
    "        doc_topic = self.gamma_\n",
    "\n",
    "        doc_lengths = [len(doc) for doc_no, doc in enumerate(corpus)]\n",
    "\n",
    "        term_frequency = np.zeros(len(self.id2word))\n",
    "        for doc_no, doc in enumerate(corpus):\n",
    "            for pair in doc:\n",
    "                term_frequency[pair[0]] += pair[1]\n",
    "\n",
    "        vocab = [self.id2word[i] for i in range(0, len(self.id2word))]\n",
    "        # returns numpy arrays for doc_topic proportions, topic_term proportions, and document_lengths, term_frequency.\n",
    "        # these should be passed to the `pyLDAvis.prepare` method to visualise one time-slice of DTM topics.\n",
    "        return doc_topic, topic_term, doc_lengths, term_frequency, vocab\n",
    "\n",
    "    def dtm_coherence(self, time, num_words=20):\n",
    "        \"\"\"Get all topics of a particular time-slice without probability values for it to be used.\n",
    "        For either \"u_mass\" or \"c_v\" coherence.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_words : int\n",
    "            Number of words.\n",
    "        time : int\n",
    "            Timestamp\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        coherence_topics : list of list of str\n",
    "            All topics of a particular time-slice without probability values for it to be used.\n",
    "\n",
    "        Warnings\n",
    "        --------\n",
    "        TODO: because of print format right now can only return for 1st time-slice, should we fix the coherence\n",
    "        printing or make changes to the print statements to mirror DTM python?\n",
    "\n",
    "        \"\"\"\n",
    "        coherence_topics = []\n",
    "        for topic_no in range(0, self.num_topics):\n",
    "            topic = self.show_topic(topicid=topic_no, time=time, num_words=num_words)\n",
    "            coherence_topic = []\n",
    "            for prob, word in topic:\n",
    "                coherence_topic.append(word)\n",
    "            coherence_topics.append(coherence_topic)\n",
    "\n",
    "        return coherence_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0790c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from gensim.models.wrappers import DtmModel\n",
    "#from gensim import corpora, utils\n",
    "#from gensim.models.wrappers import DtmModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle  # Import pickle for serialization\n",
    "#from collections import Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cb7a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preparing documents...\n",
      "Preparing time slices...\n",
      "Creating dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/b80y7bln16n67zwg12k_n4mc0000gn/T/ipykernel_5360/3125479973.py:12: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['YearMonth'] = df['Date'].dt.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating corpus...\n",
      "Training DTM model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining DTM model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m DtmModel(dtm_path, corpus\u001b[38;5;241m=\u001b[39mcorpus, time_slices\u001b[38;5;241m=\u001b[39mtime_slices, num_topics\u001b[38;5;241m=\u001b[39mnum_topics, id2word\u001b[38;5;241m=\u001b[39mdictionary)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Save the model to disk\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m, in \u001b[0;36mDtmModel.__init__\u001b[0;34m(self, dtm_path, corpus, time_slices, mode, model, num_topics, id2word, prefix, lda_sequence_min_iter, lda_sequence_max_iter, lda_max_em_iter, alpha, top_chain_var, rng_seed, initialize_lda)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfluences_time \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(corpus, time_slices, mode, model)\n",
      "Cell \u001b[0;32mIn[1], line 328\u001b[0m, in \u001b[0;36mDtmModel.train\u001b[0;34m(self, corpus, time_slices, mode, model)\u001b[0m\n\u001b[1;32m    326\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtm_path] \u001b[38;5;241m+\u001b[39m arguments\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    327\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning command \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cmd)\n\u001b[0;32m--> 328\u001b[0m check_output(args\u001b[38;5;241m=\u001b[39mcmd, stderr\u001b[38;5;241m=\u001b[39mPIPE)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem_steps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfem_steps())\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_ss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflda_ss())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:1886\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1884\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMMAND: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, popenargs, kwargs)\n\u001b[1;32m   1885\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(stdout\u001b[38;5;241m=\u001b[39mstdout, \u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1886\u001b[0m output, unused_err \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m   1887\u001b[0m retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load my preprocessed dataset\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Data/coal_data_preprocessed.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Preparing documents\n",
    "print(\"Preparing documents...\")\n",
    "documents = [doc.split() for doc in df['Processed_Article'].dropna()]\n",
    "\n",
    "# Prepare timestamps for monthly time slices\n",
    "print(\"Preparing time slices...\")\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "timestamps = df['YearMonth'].dt.strftime('%Y-%m').unique().tolist()\n",
    "time_slices = [df['YearMonth'].value_counts()[period] for period in sorted(df['YearMonth'].unique())]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "print(\"Creating dictionary...\")\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Create a corpus from the dictionary and documents\n",
    "print(\"Creating corpus...\")\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Define the path to the DTM binary\n",
    "dtm_path = '/Users/giacomoraederscheidt/dtm/dtm/main'\n",
    "\n",
    "# Initialize and train the DTM model\n",
    "print(\"Training DTM model...\")\n",
    "num_topics = 17\n",
    "model = DtmModel(dtm_path, corpus=corpus, time_slices=time_slices, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Save the model to disk\n",
    "print(\"Saving model...\")\n",
    "with open('dtm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load the model from disk\n",
    "print(\"Loading model from disk...\")\n",
    "with open('dtm_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Displaying topics\n",
    "print(\"Displaying topics...\")\n",
    "topics = loaded_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Loop over each time slice to gather topic information\n",
    "print(\"Gathering topic information over time...\")\n",
    "all_topics = []\n",
    "for time_slice in range(len(time_slices)):\n",
    "    print(f\"Processing time slice {time_slice + 1}/{len(time_slices)}...\")\n",
    "    for topic_num in range(num_topics):\n",
    "        top_words = loaded_model.show_topic(topicid=topic_num, time=time_slice, topn=20)\n",
    "        topic_info = {\n",
    "            \"TimeSlice\": time_slice,\n",
    "            \"TopicNum\": topic_num,\n",
    "            \"Words\": [word for word, _ in top_words],\n",
    "            \"Weights\": [weight for _, weight in top_words]\n",
    "        }\n",
    "        all_topics.append(topic_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "topics_df = pd.DataFrame(all_topics)\n",
    "\n",
    "# Save the DataFrame to a CSV for further analysis\n",
    "print(\"Saving topics dataframe...\")\n",
    "topics_df.to_csv(\"dtm_topics_over_time.csv\", index=False)\n",
    "print(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ed11f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Sampling subset of data...\n",
      "Preparing documents...\n",
      "Preparing time slices...\n",
      "Creating dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/b80y7bln16n67zwg12k_n4mc0000gn/T/ipykernel_7932/2172987748.py:22: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['YearMonth'] = df['Date'].dt.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating corpus...\n",
      "Training DTM model...\n",
      "Saving model...\n",
      "Loading model from disk...\n",
      "Displaying topics...\n",
      "0.021*percent + 0.017*price + 0.011*gold + 0.011*south + 0.011*product + 0.010*increas + 0.010*market + 0.010*sa + 0.009*share + 0.008*mine\n",
      "0.029*climat + 0.019*carbon + 0.018*chang + 0.018*africa + 0.016*energi + 0.016*emiss + 0.015*south + 0.012*countri + 0.010*world + 0.010*global\n",
      "0.023*compani + 0.015*mine + 0.013*billion + 0.011*group + 0.011*anglo + 0.009*list + 0.009*invest + 0.008*busi + 0.008*oper + 0.008*share\n",
      "0.017*court + 0.012*depart + 0.012*order + 0.010*polic + 0.009*transasia + 0.009*hospit + 0.008*truck + 0.007*tender + 0.006*municip + 0.006*investig\n",
      "0.050*energi + 0.027*power + 0.023*nuclear + 0.016*electr + 0.015*renew + 0.013*south + 0.012*africa + 0.011*gener + 0.011*solar + 0.010*project\n",
      "0.082*eskom + 0.053*power + 0.027*suppli + 0.023*station + 0.020*electr + 0.015*shed + 0.015*load + 0.012*util + 0.009*medupi + 0.007*gener\n",
      "0.022*busi + 0.014*compani + 0.011*properti + 0.010*sasol + 0.007*steel + 0.007*pay + 0.006*khumalo + 0.006*cape + 0.006*employe + 0.006*work\n",
      "0.013*compani + 0.012*gupta + 0.011*mine + 0.008*court + 0.008*director + 0.007*investig + 0.007*state + 0.007*contract + 0.007*deal + 0.006*eskom\n",
      "0.022*eskom + 0.018*price + 0.017*cost + 0.014*govern + 0.011*fund + 0.010*increas + 0.010*debt + 0.008*tariff + 0.008*need + 0.008*pay\n",
      "0.042*mine + 0.042*water + 0.014*environment + 0.014*area + 0.012*depart + 0.012*compani + 0.011*africa + 0.010*commun + 0.010*oper + 0.007*river\n",
      "0.019*percent + 0.018*price + 0.012*gold + 0.011*south + 0.010*product + 0.010*increas + 0.010*market + 0.010*sa + 0.009*share + 0.008*growth\n",
      "0.029*climat + 0.020*carbon + 0.019*chang + 0.018*africa + 0.017*emiss + 0.017*energi + 0.015*south + 0.012*countri + 0.010*world + 0.010*global\n",
      "0.024*compani + 0.015*mine + 0.013*billion + 0.011*group + 0.011*anglo + 0.009*list + 0.009*invest + 0.008*busi + 0.008*share + 0.008*oper\n",
      "0.018*court + 0.011*order + 0.011*depart + 0.010*transasia + 0.010*polic + 0.008*truck + 0.008*hospit + 0.007*tender + 0.006*municip + 0.006*ensafrica\n",
      "0.050*energi + 0.027*power + 0.024*nuclear + 0.016*electr + 0.015*renew + 0.013*south + 0.012*africa + 0.011*gener + 0.010*solar + 0.010*project\n",
      "0.082*eskom + 0.053*power + 0.027*suppli + 0.023*station + 0.019*electr + 0.015*shed + 0.015*load + 0.012*util + 0.010*medupi + 0.007*gener\n",
      "0.022*busi + 0.014*compani + 0.011*properti + 0.010*sasol + 0.007*steel + 0.007*pay + 0.006*cape + 0.006*khumalo + 0.006*employe + 0.006*work\n",
      "0.013*compani + 0.012*gupta + 0.011*mine + 0.008*court + 0.008*director + 0.007*investig + 0.007*state + 0.007*contract + 0.007*deal + 0.006*eskom\n",
      "0.021*eskom + 0.018*price + 0.017*cost + 0.014*govern + 0.011*fund + 0.010*increas + 0.010*debt + 0.008*tariff + 0.008*need + 0.008*pay\n",
      "0.043*mine + 0.040*water + 0.015*environment + 0.014*area + 0.012*depart + 0.012*compani + 0.011*commun + 0.010*africa + 0.010*oper + 0.007*river\n",
      "0.018*price + 0.014*percent + 0.012*gold + 0.011*south + 0.011*sa + 0.011*increas + 0.010*product + 0.010*market + 0.010*share + 0.009*growth\n",
      "0.028*climat + 0.021*carbon + 0.019*chang + 0.017*emiss + 0.017*africa + 0.017*energi + 0.015*south + 0.012*countri + 0.010*world + 0.010*transit\n",
      "0.024*compani + 0.015*mine + 0.014*billion + 0.011*group + 0.010*anglo + 0.009*list + 0.009*invest + 0.008*busi + 0.008*share + 0.008*oper\n",
      "0.018*court + 0.011*order + 0.010*depart + 0.010*transasia + 0.010*polic + 0.009*truck + 0.007*hospit + 0.007*municip + 0.006*ensafrica + 0.006*tender\n",
      "0.049*energi + 0.028*power + 0.024*nuclear + 0.016*electr + 0.015*renew + 0.012*south + 0.012*africa + 0.011*gener + 0.010*solar + 0.010*project\n",
      "0.082*eskom + 0.053*power + 0.026*suppli + 0.024*station + 0.019*electr + 0.014*load + 0.014*shed + 0.012*util + 0.010*medupi + 0.007*gener\n",
      "0.021*busi + 0.014*compani + 0.011*properti + 0.010*sasol + 0.008*steel + 0.007*pay + 0.007*cape + 0.006*employe + 0.006*khumalo + 0.006*work\n",
      "0.013*compani + 0.012*gupta + 0.011*mine + 0.008*court + 0.008*director + 0.007*investig + 0.007*deal + 0.007*state + 0.007*contract + 0.007*eskom\n",
      "0.020*eskom + 0.018*price + 0.017*cost + 0.014*govern + 0.011*fund + 0.010*increas + 0.009*debt + 0.009*tariff + 0.008*need + 0.008*pay\n",
      "0.045*mine + 0.037*water + 0.015*environment + 0.014*area + 0.012*depart + 0.012*compani + 0.011*commun + 0.010*africa + 0.010*oper + 0.007*project\n",
      "0.018*price + 0.012*gold + 0.011*sa + 0.011*share + 0.011*increas + 0.011*south + 0.010*percent + 0.010*product + 0.010*market + 0.009*growth\n",
      "0.028*climat + 0.023*carbon + 0.019*chang + 0.018*emiss + 0.017*energi + 0.016*africa + 0.014*south + 0.012*countri + 0.010*transit + 0.010*world\n",
      "0.024*compani + 0.015*mine + 0.015*billion + 0.010*group + 0.010*anglo + 0.009*list + 0.009*invest + 0.009*share + 0.009*busi + 0.008*oper\n",
      "0.017*court + 0.010*order + 0.010*truck + 0.010*polic + 0.010*depart + 0.009*transasia + 0.007*municip + 0.006*hospit + 0.006*road + 0.006*offic\n",
      "0.047*energi + 0.029*power + 0.023*nuclear + 0.016*electr + 0.015*renew + 0.012*south + 0.012*africa + 0.011*gener + 0.010*solar + 0.010*project\n",
      "0.082*eskom + 0.053*power + 0.025*suppli + 0.024*station + 0.018*electr + 0.013*load + 0.013*shed + 0.011*util + 0.011*medupi + 0.007*gener\n",
      "0.021*busi + 0.014*compani + 0.011*properti + 0.011*sasol + 0.008*steel + 0.007*cape + 0.006*pay + 0.006*employe + 0.006*work + 0.005*employ\n",
      "0.013*compani + 0.012*gupta + 0.011*mine + 0.008*court + 0.008*deal + 0.008*director + 0.007*investig + 0.007*state + 0.007*contract + 0.007*eskom\n",
      "0.019*eskom + 0.018*price + 0.017*cost + 0.015*govern + 0.010*increas + 0.010*fund + 0.009*tariff + 0.009*debt + 0.008*need + 0.008*pay\n",
      "0.046*mine + 0.034*water + 0.015*environment + 0.014*area + 0.013*compani + 0.012*depart + 0.011*commun + 0.010*africa + 0.010*oper + 0.007*project\n",
      "0.019*price + 0.012*share + 0.012*gold + 0.011*sa + 0.011*increas + 0.011*south + 0.010*market + 0.010*product + 0.009*growth + 0.008*earn\n",
      "0.028*climat + 0.026*carbon + 0.020*emiss + 0.019*chang + 0.017*energi + 0.015*africa + 0.013*south + 0.012*countri + 0.010*transit + 0.010*world\n",
      "0.024*compani + 0.016*billion + 0.015*mine + 0.010*anglo + 0.010*group + 0.009*list + 0.009*share + 0.009*busi + 0.009*invest + 0.008*oper\n",
      "0.016*court + 0.011*truck + 0.010*polic + 0.009*order + 0.009*depart + 0.008*transasia + 0.007*municip + 0.006*road + 0.006*hospit + 0.006*offic\n",
      "0.045*energi + 0.030*power + 0.023*nuclear + 0.016*electr + 0.014*renew + 0.012*south + 0.011*gener + 0.011*africa + 0.010*project + 0.010*solar\n",
      "0.081*eskom + 0.053*power + 0.025*station + 0.024*suppli + 0.017*electr + 0.012*load + 0.012*medupi + 0.012*shed + 0.011*util + 0.007*gener\n",
      "0.020*busi + 0.014*compani + 0.011*sasol + 0.011*properti + 0.009*steel + 0.007*cape + 0.006*pay + 0.006*employe + 0.006*work + 0.005*employ\n",
      "0.013*compani + 0.013*gupta + 0.011*mine + 0.009*deal + 0.008*court + 0.008*director + 0.007*contract + 0.007*investig + 0.007*state + 0.007*eskom\n",
      "0.018*eskom + 0.018*price + 0.017*cost + 0.015*govern + 0.011*increas + 0.010*fund + 0.010*tariff + 0.009*debt + 0.008*need + 0.008*pay\n",
      "0.047*mine + 0.031*water + 0.016*environment + 0.014*area + 0.013*compani + 0.012*depart + 0.011*commun + 0.010*africa + 0.009*oper + 0.007*project\n",
      "Gathering topic information over time...\n",
      "Processing time slice 1/169...\n",
      "Processing time slice 2/169...\n",
      "Processing time slice 3/169...\n",
      "Processing time slice 4/169...\n",
      "Processing time slice 5/169...\n",
      "Processing time slice 6/169...\n",
      "Processing time slice 7/169...\n",
      "Processing time slice 8/169...\n",
      "Processing time slice 9/169...\n",
      "Processing time slice 10/169...\n",
      "Processing time slice 11/169...\n",
      "Processing time slice 12/169...\n",
      "Processing time slice 13/169...\n",
      "Processing time slice 14/169...\n",
      "Processing time slice 15/169...\n",
      "Processing time slice 16/169...\n",
      "Processing time slice 17/169...\n",
      "Processing time slice 18/169...\n",
      "Processing time slice 19/169...\n",
      "Processing time slice 20/169...\n",
      "Processing time slice 21/169...\n",
      "Processing time slice 22/169...\n",
      "Processing time slice 23/169...\n",
      "Processing time slice 24/169...\n",
      "Processing time slice 25/169...\n",
      "Processing time slice 26/169...\n",
      "Processing time slice 27/169...\n",
      "Processing time slice 28/169...\n",
      "Processing time slice 29/169...\n",
      "Processing time slice 30/169...\n",
      "Processing time slice 31/169...\n",
      "Processing time slice 32/169...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time slice 33/169...\n",
      "Processing time slice 34/169...\n",
      "Processing time slice 35/169...\n",
      "Processing time slice 36/169...\n",
      "Processing time slice 37/169...\n",
      "Processing time slice 38/169...\n",
      "Processing time slice 39/169...\n",
      "Processing time slice 40/169...\n",
      "Processing time slice 41/169...\n",
      "Processing time slice 42/169...\n",
      "Processing time slice 43/169...\n",
      "Processing time slice 44/169...\n",
      "Processing time slice 45/169...\n",
      "Processing time slice 46/169...\n",
      "Processing time slice 47/169...\n",
      "Processing time slice 48/169...\n",
      "Processing time slice 49/169...\n",
      "Processing time slice 50/169...\n",
      "Processing time slice 51/169...\n",
      "Processing time slice 52/169...\n",
      "Processing time slice 53/169...\n",
      "Processing time slice 54/169...\n",
      "Processing time slice 55/169...\n",
      "Processing time slice 56/169...\n",
      "Processing time slice 57/169...\n",
      "Processing time slice 58/169...\n",
      "Processing time slice 59/169...\n",
      "Processing time slice 60/169...\n",
      "Processing time slice 61/169...\n",
      "Processing time slice 62/169...\n",
      "Processing time slice 63/169...\n",
      "Processing time slice 64/169...\n",
      "Processing time slice 65/169...\n",
      "Processing time slice 66/169...\n",
      "Processing time slice 67/169...\n",
      "Processing time slice 68/169...\n",
      "Processing time slice 69/169...\n",
      "Processing time slice 70/169...\n",
      "Processing time slice 71/169...\n",
      "Processing time slice 72/169...\n",
      "Processing time slice 73/169...\n",
      "Processing time slice 74/169...\n",
      "Processing time slice 75/169...\n",
      "Processing time slice 76/169...\n",
      "Processing time slice 77/169...\n",
      "Processing time slice 78/169...\n",
      "Processing time slice 79/169...\n",
      "Processing time slice 80/169...\n",
      "Processing time slice 81/169...\n",
      "Processing time slice 82/169...\n",
      "Processing time slice 83/169...\n",
      "Processing time slice 84/169...\n",
      "Processing time slice 85/169...\n",
      "Processing time slice 86/169...\n",
      "Processing time slice 87/169...\n",
      "Processing time slice 88/169...\n",
      "Processing time slice 89/169...\n",
      "Processing time slice 90/169...\n",
      "Processing time slice 91/169...\n",
      "Processing time slice 92/169...\n",
      "Processing time slice 93/169...\n",
      "Processing time slice 94/169...\n",
      "Processing time slice 95/169...\n",
      "Processing time slice 96/169...\n",
      "Processing time slice 97/169...\n",
      "Processing time slice 98/169...\n",
      "Processing time slice 99/169...\n",
      "Processing time slice 100/169...\n",
      "Processing time slice 101/169...\n",
      "Processing time slice 102/169...\n",
      "Processing time slice 103/169...\n",
      "Processing time slice 104/169...\n",
      "Processing time slice 105/169...\n",
      "Processing time slice 106/169...\n",
      "Processing time slice 107/169...\n",
      "Processing time slice 108/169...\n",
      "Processing time slice 109/169...\n",
      "Processing time slice 110/169...\n",
      "Processing time slice 111/169...\n",
      "Processing time slice 112/169...\n",
      "Processing time slice 113/169...\n",
      "Processing time slice 114/169...\n",
      "Processing time slice 115/169...\n",
      "Processing time slice 116/169...\n",
      "Processing time slice 117/169...\n",
      "Processing time slice 118/169...\n",
      "Processing time slice 119/169...\n",
      "Processing time slice 120/169...\n",
      "Processing time slice 121/169...\n",
      "Processing time slice 122/169...\n",
      "Processing time slice 123/169...\n",
      "Processing time slice 124/169...\n",
      "Processing time slice 125/169...\n",
      "Processing time slice 126/169...\n",
      "Processing time slice 127/169...\n",
      "Processing time slice 128/169...\n",
      "Processing time slice 129/169...\n",
      "Processing time slice 130/169...\n",
      "Processing time slice 131/169...\n",
      "Processing time slice 132/169...\n",
      "Processing time slice 133/169...\n",
      "Processing time slice 134/169...\n",
      "Processing time slice 135/169...\n",
      "Processing time slice 136/169...\n",
      "Processing time slice 137/169...\n",
      "Processing time slice 138/169...\n",
      "Processing time slice 139/169...\n",
      "Processing time slice 140/169...\n",
      "Processing time slice 141/169...\n",
      "Processing time slice 142/169...\n",
      "Processing time slice 143/169...\n",
      "Processing time slice 144/169...\n",
      "Processing time slice 145/169...\n",
      "Processing time slice 146/169...\n",
      "Processing time slice 147/169...\n",
      "Processing time slice 148/169...\n",
      "Processing time slice 149/169...\n",
      "Processing time slice 150/169...\n",
      "Processing time slice 151/169...\n",
      "Processing time slice 152/169...\n",
      "Processing time slice 153/169...\n",
      "Processing time slice 154/169...\n",
      "Processing time slice 155/169...\n",
      "Processing time slice 156/169...\n",
      "Processing time slice 157/169...\n",
      "Processing time slice 158/169...\n",
      "Processing time slice 159/169...\n",
      "Processing time slice 160/169...\n",
      "Processing time slice 161/169...\n",
      "Processing time slice 162/169...\n",
      "Processing time slice 163/169...\n",
      "Processing time slice 164/169...\n",
      "Processing time slice 165/169...\n",
      "Processing time slice 166/169...\n",
      "Processing time slice 167/169...\n",
      "Processing time slice 168/169...\n",
      "Processing time slice 169/169...\n",
      "Saving topics dataframe...\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "## For the case everything takes too long... Just do it on a subset\n",
    "\n",
    "# Load my preprocessed dataset\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Data/coal_data_preprocessed.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "# Sample a small subset of the data\n",
    "print(\"Sampling subset of data...\")\n",
    "#sample_fraction = 1  # Adjust the fraction as needed\n",
    "## For 5% of the dataset (ca 430 articles) it took 40 minutes to train. However less min and max iterations (2 and 10 instead\n",
    "## of 6 and 20. For 20%, it took around 2,5 hours -> For all (with 3, and 15) \n",
    "#df_subset = df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Preparing documents\n",
    "print(\"Preparing documents...\")\n",
    "documents = [doc.split() for doc in df['Processed_Article'].dropna()]\n",
    "\n",
    "# Prepare timestamps for monthly time slices\n",
    "print(\"Preparing time slices...\")\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "timestamps = df['YearMonth'].dt.strftime('%Y-%m').unique().tolist()\n",
    "time_slices = [df['YearMonth'].value_counts()[period] for period in sorted(df['YearMonth'].unique())]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "print(\"Creating dictionary...\")\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Create a corpus from the dictionary and documents\n",
    "print(\"Creating corpus...\")\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Define the path to the DTM binary\n",
    "dtm_path = '/Users/giacomoraederscheidt/dtm/dtm/main'\n",
    "\n",
    "# Initialize and train the DTM model\n",
    "print(\"Training DTM model...\")\n",
    "num_topics = 20\n",
    "model = DtmModel(dtm_path, corpus=corpus, time_slices=time_slices, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Save the model to disk\n",
    "print(\"Saving model...\")\n",
    "with open('dtm_model_subset_100_percent.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load the model from disk\n",
    "print(\"Loading model from disk...\")\n",
    "with open('dtm_model_subset_100_percent.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Displaying topics\n",
    "print(\"Displaying topics...\")\n",
    "topics = loaded_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Loop over each time slice to gather topic information\n",
    "print(\"Gathering topic information over time...\")\n",
    "all_topics = []\n",
    "for time_slice in range(len(time_slices)):\n",
    "    print(f\"Processing time slice {time_slice + 1}/{len(time_slices)}...\")\n",
    "    for topic_num in range(num_topics):\n",
    "        top_words = loaded_model.show_topic(topicid=topic_num, time=time_slice, topn=20)\n",
    "        topic_info = {\n",
    "            \"TimeSlice\": time_slice,\n",
    "            \"TopicNum\": topic_num,\n",
    "            \"Words\": [word for word, _ in top_words],\n",
    "            \"Weights\": [weight for _, weight in top_words]\n",
    "        }\n",
    "        all_topics.append(topic_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "topics_df = pd.DataFrame(all_topics)\n",
    "\n",
    "# Save the DataFrame to a CSV for further analysis\n",
    "print(\"Saving topics dataframe...\")\n",
    "topics_df.to_csv(\"dtm_topics_over_time_subset_100_percent.csv\", index=False)\n",
    "print(\"Process completed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395bd378",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdates\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmdates\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming topic_names is a dictionary mapping topic indices to topic names\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(topic_names)\n\u001b[1;32m      8\u001b[0m topic_shares_over_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(time_slices), num_topics))\n\u001b[1;32m     10\u001b[0m current_document \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_names' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Assuming topic_names is a dictionary mapping topic indices to topic names\n",
    "num_topics = len(topic_names)\n",
    "topic_shares_over_time = np.zeros((len(time_slices), num_topics))\n",
    "\n",
    "current_document = 0\n",
    "for time_slice_index, num_docs in enumerate(time_slices):\n",
    "    for _ in range(num_docs):\n",
    "        topic_shares_over_time[time_slice_index, :] += model.gamma_[current_document, :]\n",
    "        current_document += 1\n",
    "\n",
    "    # Normalize by number of documents to get the average topic distribution for this time slice\n",
    "    topic_shares_over_time[time_slice_index, :] /= num_docs\n",
    "\n",
    "# Rolling averages\n",
    "window_size = 3  # 12 months for 1 year\n",
    "rolling_topic_shares = np.array([pd.Series(topic_shares_over_time[:, i]).rolling(window=window_size).mean()\n",
    "                                 for i in range(num_topics)]).T\n",
    "\n",
    "# Generate date range from January 2013 to October 2023\n",
    "dates = pd.date_range(start='2010-01-01', periods=len(time_slices), freq='MS')\n",
    "\n",
    "# Determine the peak year for each topic\n",
    "peak_years = {}\n",
    "for topic_idx, topic_name in topic_names.items():\n",
    "    topic_data = rolling_topic_shares[:, topic_idx]\n",
    "    # Find the index of the maximum value ignoring NaNs\n",
    "    peak_index = np.nanargmax(topic_data)\n",
    "    peak_years[topic_idx] = dates[peak_index].year if peak_index >= 0 else None\n",
    "\n",
    "# Now print the peak year for each topic\n",
    "for topic_idx, year in peak_years.items():\n",
    "    if year is not None:\n",
    "        print(f\"Topic: {topic_names[topic_idx]}, Peak Year: {year}\")\n",
    "    else:\n",
    "        print(f\"Topic: {topic_names[topic_idx]} has no peak year due to insufficient data.\")\n",
    "\n",
    "# Order topics by peak year\n",
    "ordered_topics = sorted(peak_years.keys(), key=lambda t: peak_years[t])\n",
    "\n",
    "# Plotting\n",
    "fig_width = 8.27  # A4 width in inches\n",
    "fig_height = 11.69  # A4 height in inches\n",
    "fig, axes = plt.subplots(nrows=num_topics, ncols=1, figsize=(fig_width, fig_height), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(left=0.15, right=0.85, top=0.95, bottom=0.05, hspace=0.25)\n",
    "\n",
    "for idx, topic_idx in enumerate(ordered_topics):\n",
    "    ax = axes[idx]\n",
    "    data = rolling_topic_shares[:, topic_idx]\n",
    "    ax.fill_between(dates, 0, data, label=topic_names[topic_idx], alpha=0.5)\n",
    "    ax.text(1.05, 0.5, topic_names[topic_idx], transform=ax.transAxes, va='center', ha='left', fontsize=8)\n",
    "\n",
    "    # Setting the x-axis for years only\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(1))  # Set major ticks to every year\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Show only the year\n",
    "\n",
    "# Set common x-axis labels\n",
    "axes[-1].set_xlabel('Year (January 1st)')\n",
    "\n",
    "# Set common y-axis label\n",
    "fig.text(0.05, 0.5, 'Topic shares (rolling averages)', va='center', rotation='vertical', fontsize=10)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Text-Tables/topic_shares_rolling_average.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65734d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
