{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01129102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "import os\n",
    "from subprocess import PIPE\n",
    "import numpy as np\n",
    "\n",
    "from gensim import utils, corpora, matutils\n",
    "from gensim.utils import check_output\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DtmModel(utils.SaveLoad):\n",
    "    \"\"\"Python wrapper using `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_.\n",
    "\n",
    "    Communication between DTM and Python takes place by passing around data files on disk and executing\n",
    "    the DTM binary as a subprocess.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    This is **only** python wrapper for `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_,\n",
    "    you need to install original implementation first and pass the path to binary to ``dtm_path``.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dtm_path, corpus=None, time_slices=None, mode='fit', model='dtm', num_topics=100,\n",
    "                 id2word=None, prefix=None, lda_sequence_min_iter=6, lda_sequence_max_iter=20, lda_max_em_iter=10,\n",
    "                 alpha=0.01, top_chain_var=0.005, rng_seed=0, initialize_lda=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dtm_path : str\n",
    "            Path to the dtm binary, e.g. `/home/username/dtm/dtm/main`.\n",
    "        corpus : iterable of iterable of (int, int)\n",
    "            Collection of texts in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "        mode : {'fit', 'time'}, optional\n",
    "            Controls the mode of the mode: 'fit' is for training, 'time' for analyzing documents through time\n",
    "            according to a DTM, basically a held out set.\n",
    "        model : {'fixed', 'dtm'}, optional\n",
    "            Control model that will be runned: 'fixed' is for DIM and 'dtm' for DTM.\n",
    "        num_topics : int, optional\n",
    "            Number of topics.\n",
    "        id2word : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
    "            Mapping between tokens ids and words from corpus, if not specified - will be inferred from `corpus`.\n",
    "        prefix : str, optional\n",
    "            Prefix for produced temporary files.\n",
    "        lda_sequence_min_iter : int, optional\n",
    "             Min iteration of LDA.\n",
    "        lda_sequence_max_iter : int, optional\n",
    "            Max iteration of LDA.\n",
    "        lda_max_em_iter : int, optional\n",
    "             Max em optimization iterations in LDA.\n",
    "        alpha : int, optional\n",
    "            Hyperparameter that affects sparsity of the document-topics for the LDA models in each timeslice.\n",
    "        top_chain_var : float, optional\n",
    "            This hyperparameter controls one of the key aspect of topic evolution which is the speed at which\n",
    "            these topics evolve. A smaller top_chain_var leads to similar word distributions over multiple timeslice.\n",
    "\n",
    "        rng_seed : int, optional\n",
    "             Random seed.\n",
    "        initialize_lda : bool, optional\n",
    "             If True - initialize DTM with LDA.\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(dtm_path):\n",
    "            raise ValueError(\"dtm_path must point to the binary file, not to a folder\")\n",
    "\n",
    "        self.dtm_path = dtm_path\n",
    "        self.id2word = id2word\n",
    "        if self.id2word is None:\n",
    "            logger.warning(\"no word id mapping provided; initializing from corpus, assuming identity\")\n",
    "            self.id2word = utils.dict_from_corpus(corpus)\n",
    "            self.num_terms = len(self.id2word)\n",
    "        else:\n",
    "            self.num_terms = 0 if not self.id2word else 1 + max(self.id2word.keys())\n",
    "        if self.num_terms == 0:\n",
    "            raise ValueError(\"cannot compute DTM over an empty collection (no terms)\")\n",
    "        self.num_topics = num_topics\n",
    "\n",
    "        try:\n",
    "            lencorpus = len(corpus)\n",
    "        except TypeError:\n",
    "            logger.warning(\"input corpus stream has no len(); counting documents\")\n",
    "            lencorpus = sum(1 for _ in corpus)\n",
    "        if lencorpus == 0:\n",
    "            raise ValueError(\"cannot compute DTM over an empty corpus\")\n",
    "        if model == \"fixed\" and any(not text for text in corpus):\n",
    "            raise ValueError(\"\"\"There is a text without words in the input corpus.\n",
    "                    This breaks method='fixed' (The DIM model).\"\"\")\n",
    "        if lencorpus != sum(time_slices):\n",
    "            raise ValueError(\n",
    "                \"mismatched timeslices %{slices} for corpus of len {clen}\"\n",
    "                .format(slices=sum(time_slices), clen=lencorpus)\n",
    "            )\n",
    "        self.lencorpus = lencorpus\n",
    "        if prefix is None:\n",
    "            rand_prefix = hex(random.randint(0, 0xffffff))[2:] + '_'\n",
    "            prefix = os.path.join(tempfile.gettempdir(), rand_prefix)\n",
    "\n",
    "        self.prefix = prefix\n",
    "        self.time_slices = time_slices\n",
    "        self.lda_sequence_min_iter = int(lda_sequence_min_iter)\n",
    "        self.lda_sequence_max_iter = int(lda_sequence_max_iter)\n",
    "        self.lda_max_em_iter = int(lda_max_em_iter)\n",
    "        self.alpha = alpha\n",
    "        self.top_chain_var = top_chain_var\n",
    "        self.rng_seed = rng_seed\n",
    "        self.initialize_lda = str(initialize_lda).lower()\n",
    "\n",
    "        self.lambda_ = None\n",
    "        self.obs_ = None\n",
    "        self.lhood_ = None\n",
    "        self.gamma_ = None\n",
    "        self.init_alpha = None\n",
    "        self.init_beta = None\n",
    "        self.init_ss = None\n",
    "        self.em_steps = []\n",
    "        self.influences_time = []\n",
    "\n",
    "        if corpus is not None:\n",
    "            self.train(corpus, time_slices, mode, model)\n",
    "\n",
    "    def fout_liklihoods(self):\n",
    "        \"\"\"Get path to temporary lhood data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to lhood data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'lhoods.dat'\n",
    "\n",
    "    def fout_gamma(self):\n",
    "        \"\"\"Get path to temporary gamma data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to gamma data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'gam.dat'\n",
    "\n",
    "    def fout_prob(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'topic-{i}-var-e-log-prob.dat'\n",
    "\n",
    "    def fout_observations(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'topic-{i}-var-obs.dat'\n",
    "\n",
    "    def fout_influence(self):\n",
    "        \"\"\"Get template of path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/lda-seq/' + 'influence_time-{i}'\n",
    "\n",
    "    def foutname(self):\n",
    "        \"\"\"Get path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out'\n",
    "\n",
    "    def fem_steps(self):\n",
    "        \"\"\"Get path to temporary em_step data file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to em_step data file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'em_log.dat'\n",
    "\n",
    "    def finit_alpha(self):\n",
    "        \"\"\"Get path to initially trained lda alpha file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initially trained lda alpha file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda.alpha'\n",
    "\n",
    "    def finit_beta(self):\n",
    "        \"\"\"Get path to initially trained lda beta file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initially trained lda beta file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda.beta'\n",
    "\n",
    "    def flda_ss(self):\n",
    "        \"\"\"Get path to initial lda binary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to initial lda binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train_out/' + 'initial-lda-ss.dat'\n",
    "\n",
    "    def fcorpustxt(self):\n",
    "        \"\"\"Get path to temporary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to multiple train binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train-mult.dat'\n",
    "\n",
    "    def fcorpus(self):\n",
    "        \"\"\"Get path to corpus file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to corpus file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train'\n",
    "\n",
    "    def ftimeslices(self):\n",
    "        \"\"\"Get path to time slices binary file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to time slices binary file.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.prefix + 'train-seq.dat'\n",
    "\n",
    "    def convert_input(self, corpus, time_slices):\n",
    "        \"\"\"Convert corpus into LDA-C format by :class:`~gensim.corpora.bleicorpus.BleiCorpus` and save to temp file.\n",
    "        Path to temporary file produced by :meth:`~gensim.models.wrappers.dtmmodel.DtmModel.ftimeslices`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, float)\n",
    "            Corpus in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"serializing temporary corpus to %s\", self.fcorpustxt())\n",
    "        # write out the corpus in a file format that DTM understands:\n",
    "        corpora.BleiCorpus.save_corpus(self.fcorpustxt(), corpus)\n",
    "\n",
    "        with utils.open(self.ftimeslices(), 'wb') as fout:\n",
    "            fout.write(utils.to_utf8(str(len(self.time_slices)) + \"\\n\"))\n",
    "            for sl in time_slices:\n",
    "                fout.write(utils.to_utf8(str(sl) + \"\\n\"))\n",
    "\n",
    "    def train(self, corpus, time_slices, mode, model):\n",
    "        \"\"\"Train DTM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, int)\n",
    "            Collection of texts in BoW format.\n",
    "        time_slices : list of int\n",
    "            Sequence of timestamps.\n",
    "        mode : {'fit', 'time'}, optional\n",
    "            Controls the mode of the mode: 'fit' is for training, 'time' for analyzing documents through time\n",
    "            according to a DTM, basically a held out set.\n",
    "        model : {'fixed', 'dtm'}, optional\n",
    "            Control model that will be runned: 'fixed' is for DIM and 'dtm' for DTM.\n",
    "\n",
    "        \"\"\"\n",
    "        self.convert_input(corpus, time_slices)\n",
    "\n",
    "        arguments = \\\n",
    "            \"--ntopics={p0} --model={mofrl}  --mode={p1} --initialize_lda={p2} --corpus_prefix={p3} \" \\\n",
    "            \"--outname={p4} --alpha={p5}\".format(\n",
    "                p0=self.num_topics, mofrl=model, p1=mode, p2=self.initialize_lda,\n",
    "                p3=self.fcorpus(), p4=self.foutname(), p5=self.alpha\n",
    "            )\n",
    "\n",
    "        params = \\\n",
    "            \"--lda_max_em_iter={p0} --lda_sequence_min_iter={p1}  --lda_sequence_max_iter={p2} \" \\\n",
    "            \"--top_chain_var={p3} --rng_seed={p4} \".format(\n",
    "                p0=self.lda_max_em_iter, p1=self.lda_sequence_min_iter, p2=self.lda_sequence_max_iter,\n",
    "                p3=self.top_chain_var, p4=self.rng_seed\n",
    "            )\n",
    "\n",
    "        arguments = arguments + \" \" + params\n",
    "        logger.info(\"training DTM with args %s\", arguments)\n",
    "\n",
    "        cmd = [self.dtm_path] + arguments.split()\n",
    "        logger.info(\"Running command %s\", cmd)\n",
    "        check_output(args=cmd, stderr=PIPE)\n",
    "\n",
    "        self.em_steps = np.loadtxt(self.fem_steps())\n",
    "        self.init_ss = np.loadtxt(self.flda_ss())\n",
    "\n",
    "        if self.initialize_lda:\n",
    "            self.init_alpha = np.loadtxt(self.finit_alpha())\n",
    "            self.init_beta = np.loadtxt(self.finit_beta())\n",
    "\n",
    "        self.lhood_ = np.loadtxt(self.fout_liklihoods())\n",
    "\n",
    "        # document-topic proportions\n",
    "        self.gamma_ = np.loadtxt(self.fout_gamma())\n",
    "        # cast to correct shape, gamme[5,10] is the proprtion of the 10th topic\n",
    "        # in doc 5\n",
    "        self.gamma_.shape = (self.lencorpus, self.num_topics)\n",
    "        # normalize proportions\n",
    "        self.gamma_ /= self.gamma_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        self.lambda_ = np.zeros((self.num_topics, self.num_terms * len(self.time_slices)))\n",
    "        self.obs_ = np.zeros((self.num_topics, self.num_terms * len(self.time_slices)))\n",
    "\n",
    "        for t in range(self.num_topics):\n",
    "            topic = \"%03d\" % t\n",
    "            self.lambda_[t, :] = np.loadtxt(self.fout_prob().format(i=topic))\n",
    "            self.obs_[t, :] = np.loadtxt(self.fout_observations().format(i=topic))\n",
    "        # cast to correct shape, lambda[5,10,0] is the proportion of the 10th\n",
    "        # topic in doc 5 at time 0\n",
    "        self.lambda_.shape = (self.num_topics, self.num_terms, len(self.time_slices))\n",
    "        self.obs_.shape = (self.num_topics, self.num_terms, len(self.time_slices))\n",
    "        # extract document influence on topics for each time slice\n",
    "        # influences_time[0] , influences at time 0\n",
    "        if model == 'fixed':\n",
    "            for k, t in enumerate(self.time_slices):\n",
    "                stamp = \"%03d\" % k\n",
    "                influence = np.loadtxt(self.fout_influence().format(i=stamp))\n",
    "                influence.shape = (t, self.num_topics)\n",
    "                # influence[2,5] influence of document 2 on topic 5\n",
    "                self.influences_time.append(influence)\n",
    "\n",
    "    def print_topics(self, num_topics=10, times=5, num_words=10):\n",
    "        \"\"\"Alias for :meth:`~gensim.models.wrappers.dtmmodel.DtmModel.show_topics`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics : int, optional\n",
    "            Number of topics to return, set `-1` to get all topics.\n",
    "        times : int, optional\n",
    "            Number of times.\n",
    "        num_words : int, optional\n",
    "            Number of words.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            Topics as a list of strings\n",
    "\n",
    "        \"\"\"\n",
    "        return self.show_topics(num_topics, times, num_words, log=True)\n",
    "\n",
    "    def show_topics(self, num_topics=10, times=5, num_words=10, log=False, formatted=True):\n",
    "        \"\"\"Get the `num_words` most probable words for `num_topics` number of topics at 'times' time slices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics : int, optional\n",
    "            Number of topics to return, set `-1` to get all topics.\n",
    "        times : int, optional\n",
    "            Number of times.\n",
    "        num_words : int, optional\n",
    "            Number of words.\n",
    "        log : bool, optional\n",
    "            THIS PARAMETER WILL BE IGNORED.\n",
    "        formatted : bool, optional\n",
    "            If `True` - return the topics as a list of strings, otherwise as lists of (weight, word) pairs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            Topics as a list of strings (if formatted=True) **OR**\n",
    "        list of (float, str)\n",
    "            Topics as list of (weight, word) pairs (if formatted=False)\n",
    "\n",
    "        \"\"\"\n",
    "        if num_topics < 0 or num_topics >= self.num_topics:\n",
    "            num_topics = self.num_topics\n",
    "            chosen_topics = range(num_topics)\n",
    "        else:\n",
    "            num_topics = min(num_topics, self.num_topics)\n",
    "            chosen_topics = range(num_topics)\n",
    "\n",
    "        if times < 0 or times >= len(self.time_slices):\n",
    "            times = len(self.time_slices)\n",
    "            chosen_times = range(times)\n",
    "        else:\n",
    "            times = min(times, len(self.time_slices))\n",
    "            chosen_times = range(times)\n",
    "\n",
    "        shown = []\n",
    "        for time in chosen_times:\n",
    "            for i in chosen_topics:\n",
    "                if formatted:\n",
    "                    topic = self.print_topic(i, time, topn=num_words)\n",
    "                else:\n",
    "                    topic = self.show_topic(i, time, topn=num_words)\n",
    "                shown.append(topic)\n",
    "        return shown\n",
    "\n",
    "    def show_topic(self, topicid, time, topn=50, num_words=None):\n",
    "        \"\"\"Get `num_words` most probable words for the given `topicid`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        topicid : int\n",
    "            Id of topic.\n",
    "        time : int\n",
    "            Timestamp.\n",
    "        topn : int, optional\n",
    "            Top number of topics that you'll receive.\n",
    "        num_words : int, optional\n",
    "            DEPRECATED PARAMETER, use `topn` instead.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of (float, str)\n",
    "            Sequence of probable words, as a list of `(word_probability, word)`.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_words is not None:  # deprecated num_words is used\n",
    "            warnings.warn(\"The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\")\n",
    "            topn = num_words\n",
    "\n",
    "        topics = self.lambda_[:, :, time]\n",
    "        topic = topics[topicid]\n",
    "        # likelihood to probability\n",
    "        topic = np.exp(topic)\n",
    "        # normalize to probability dist\n",
    "        topic = topic / topic.sum()\n",
    "        # sort according to prob\n",
    "        bestn = matutils.argsort(topic, topn, reverse=True)\n",
    "        beststr = [(topic[idx], self.id2word[idx]) for idx in bestn]\n",
    "        return beststr\n",
    "\n",
    "    def print_topic(self, topicid, time, topn=10, num_words=None):\n",
    "        \"\"\"Get the given topic, formatted as a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        topicid : int\n",
    "            Id of topic.\n",
    "        time : int\n",
    "            Timestamp.\n",
    "        topn : int, optional\n",
    "            Top number of topics that you'll receive.\n",
    "        num_words : int, optional\n",
    "            DEPRECATED PARAMETER, use `topn` instead.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The given topic in string format, like '0.132*someword + 0.412*otherword + ...'.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_words is not None:  # deprecated num_words is used\n",
    "            warnings.warn(\"The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\")\n",
    "            topn = num_words\n",
    "\n",
    "        return ' + '.join('%.3f*%s' % v for v in self.show_topic(topicid, time, topn=topn))\n",
    "\n",
    "    def dtm_vis(self, corpus, time):\n",
    "        \"\"\"Get data specified by pyLDAvis format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, float)\n",
    "            Collection of texts in BoW format.\n",
    "        time : int\n",
    "            Sequence of timestamp.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        All of these are needed to visualise topics for DTM for a particular time-slice via pyLDAvis.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        doc_topic : numpy.ndarray\n",
    "            Document-topic proportions.\n",
    "        topic_term : numpy.ndarray\n",
    "            Calculated term of topic suitable for pyLDAvis format.\n",
    "        doc_lengths : list of int\n",
    "            Length of each documents in corpus.\n",
    "        term_frequency : numpy.ndarray\n",
    "            Frequency of each word from vocab.\n",
    "        vocab : list of str\n",
    "            List of words from docpus.\n",
    "\n",
    "        \"\"\"\n",
    "        topic_term = np.exp(self.lambda_[:, :, time]) / np.exp(self.lambda_[:, :, time]).sum()\n",
    "        topic_term *= self.num_topics\n",
    "\n",
    "        doc_topic = self.gamma_\n",
    "\n",
    "        doc_lengths = [len(doc) for doc_no, doc in enumerate(corpus)]\n",
    "\n",
    "        term_frequency = np.zeros(len(self.id2word))\n",
    "        for doc_no, doc in enumerate(corpus):\n",
    "            for pair in doc:\n",
    "                term_frequency[pair[0]] += pair[1]\n",
    "\n",
    "        vocab = [self.id2word[i] for i in range(0, len(self.id2word))]\n",
    "        # returns numpy arrays for doc_topic proportions, topic_term proportions, and document_lengths, term_frequency.\n",
    "        # these should be passed to the `pyLDAvis.prepare` method to visualise one time-slice of DTM topics.\n",
    "        return doc_topic, topic_term, doc_lengths, term_frequency, vocab\n",
    "\n",
    "    def dtm_coherence(self, time, num_words=20):\n",
    "        \"\"\"Get all topics of a particular time-slice without probability values for it to be used.\n",
    "        For either \"u_mass\" or \"c_v\" coherence.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_words : int\n",
    "            Number of words.\n",
    "        time : int\n",
    "            Timestamp\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        coherence_topics : list of list of str\n",
    "            All topics of a particular time-slice without probability values for it to be used.\n",
    "\n",
    "        Warnings\n",
    "        --------\n",
    "        TODO: because of print format right now can only return for 1st time-slice, should we fix the coherence\n",
    "        printing or make changes to the print statements to mirror DTM python?\n",
    "\n",
    "        \"\"\"\n",
    "        coherence_topics = []\n",
    "        for topic_no in range(0, self.num_topics):\n",
    "            topic = self.show_topic(topicid=topic_no, time=time, num_words=num_words)\n",
    "            coherence_topic = []\n",
    "            for prob, word in topic:\n",
    "                coherence_topic.append(word)\n",
    "            coherence_topics.append(coherence_topic)\n",
    "\n",
    "        return coherence_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3e8add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from disk...\n",
      "Model loaded.\n",
      "Loading data...\n",
      "Preparing time slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/b80y7bln16n67zwg12k_n4mc0000gn/T/ipykernel_1833/1307193762.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['YearMonth'] = df['Date'].dt.to_period('M')\n",
      "/var/folders/b2/b80y7bln16n67zwg12k_n4mc0000gn/T/ipykernel_1833/3012349107.py:457: UserWarning: The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\n",
      "  warnings.warn(\"The parameter `num_words` is deprecated, will be removed in 4.0.0, use `topn` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words per Topic per Year:\n",
      "                      Topic  Year        0         1         2        3  \\\n",
      "0    Mining Market Dynamics  2010    price      gold   increas    share   \n",
      "1    Mining Market Dynamics  2011    price   product    market     mine   \n",
      "2    Mining Market Dynamics  2012    price   product    market   growth   \n",
      "3    Mining Market Dynamics  2013  percent     price   increas  product   \n",
      "4    Mining Market Dynamics  2014  percent     price   product  increas   \n",
      "..                      ...   ...      ...       ...       ...      ...   \n",
      "275          Coal Transport  2019     port  transnet      rail      bay   \n",
      "276          Coal Transport  2020     rail  transnet      port   export   \n",
      "277          Coal Transport  2021     port      rail  transnet   export   \n",
      "278          Coal Transport  2022     port  transnet      rail   export   \n",
      "279          Coal Transport  2023     port  transnet      ship     rail   \n",
      "\n",
      "             4              5          6              7  ...      10       11  \\\n",
      "0      product         market     growth           mine  ...  report     rate   \n",
      "1      increas           gold     growth        quarter  ...    cost   report   \n",
      "2         rate           mine    increas         commod  ...  global     gold   \n",
      "3      quarter           mine     market      yesterday  ...  demand   export   \n",
      "4         mine           rand     market           gold  ...  growth   demand   \n",
      "..         ...            ...        ...            ...  ...     ...      ...   \n",
      "275     export        richard     durban         termin  ...    oper     line   \n",
      "276  transport  infrastructur        bay        richard  ...    line     oper   \n",
      "277        bay      transport    richard  infrastructur  ...    ship   durban   \n",
      "278        bay           oper       east         termin  ...  london  richard   \n",
      "279        bay        richard  transport           oper  ...  durban   termin   \n",
      "\n",
      "          12             13       14        15       16        17  \\\n",
      "0     demand       platinum  percent      rise      end      rand   \n",
      "1     higher       industri   global      rand      ore      bank   \n",
      "2    quarter         higher     bank  platinum     rise   percent   \n",
      "3      index       platinum     rise      rose     rate      gold   \n",
      "4      share       platinum   global       ore     rate      rise   \n",
      "..       ...            ...      ...       ...      ...       ...   \n",
      "275      ton  infrastructur  contain      road  freight     cargo   \n",
      "276  freight           ship     road   railway  contain  botswana   \n",
      "277     line        freight     east      road  compani   railway   \n",
      "278      ton           line  freight   compani   durban    logist   \n",
      "279     line            ton  compani   freight  railway   harbour   \n",
      "\n",
      "                18        19  \n",
      "0             cost  industri  \n",
      "1           demand     share  \n",
      "2           export      cost  \n",
      "3            share     point  \n",
      "4            index    report  \n",
      "..             ...       ...  \n",
      "275        railway    vessel  \n",
      "276        compani      None  \n",
      "277            ton    london  \n",
      "278  infrastructur      road  \n",
      "279         logist     cargo  \n",
      "\n",
      "[280 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the model from disk\n",
    "print(\"Loading model from disk...\")\n",
    "with open('dtm_model_subset_100_percent.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Data/coal_data_preprocessed.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Prepare timestamps for monthly time slices\n",
    "print(\"Preparing time slices...\")\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "end_period = pd.Period('2023-12', freq='M')\n",
    "filtered_df = df[df['YearMonth'] <= end_period]\n",
    "\n",
    "time_slices = [filtered_df['YearMonth'].value_counts().get(period, 0) for period in sorted(filtered_df['YearMonth'].unique())]\n",
    "dates = pd.date_range(start='2010-01-01', periods=len(time_slices), freq='MS')\n",
    "\n",
    "num_topics = 20  # Set the number of topics\n",
    "topic_shares_over_time = np.zeros((len(time_slices), num_topics))\n",
    "\n",
    "# Populate the topic shares over time, accounting only for documents up to October 2023\n",
    "doc_start_index = 0\n",
    "max_index = len(model.gamma_)  # Maximum index for gamma_ matrix\n",
    "for ts_index, num_docs in enumerate(time_slices):\n",
    "    end_index = doc_start_index + num_docs\n",
    "    if end_index > max_index:\n",
    "        end_index = max_index\n",
    "    for doc_index in range(doc_start_index, end_index):\n",
    "        topic_shares_over_time[ts_index] += model.gamma_[doc_index, :]\n",
    "    doc_start_index += num_docs\n",
    "    if doc_start_index >= max_index:\n",
    "        break  # Exit if we've processed all documents\n",
    "\n",
    "# Define topic names\n",
    "topic_names = {\n",
    "    0:  \"Mining Market Dynamics\",\n",
    "    1:  \"Climate/Transition Politics & Finance\",\n",
    "    2:  \"Mining Investments\",\n",
    "    3:  \"Safety & Illegal Activities\",\n",
    "    4:  \"Energy Policies & Renewables\",\n",
    "    5:  \"Electricity Crisis\",\n",
    "    6:  \"Stopwords/False Positives 1\",\n",
    "    7:  \"State Capture & Corruption\",\n",
    "    8:  \"Eskom Crisis\",\n",
    "    9:  \"Anti-Mining Activism\",\n",
    "    10: \"Indian South Africans History\",\n",
    "    11: \"Air Pollution\",\n",
    "    12: \"Coal Labor Issues\",\n",
    "    13: \"Coal Trade Relations\",\n",
    "    14: \"Climate Change Impact\",\n",
    "    15: \"Daily Political Business\",\n",
    "    16: \"Stopwords/False Positives 2\",\n",
    "    17: \"Oil & Gas Alternatives\",\n",
    "    18: \"State Intervention in Mining\",\n",
    "    19: \"Coal Transport\"\n",
    "}\n",
    "\n",
    "exclude_words = {\"south\", \"africa\", \"xa\", \"sa\"} # Later do it beforehand!! \n",
    "\n",
    "\n",
    "# Extract the top 10 words per topic for each year\n",
    "top_words_per_topic_year = {}\n",
    "num_words = 20  # Top 10 words\n",
    "\n",
    "for topic_id in range(num_topics):\n",
    "    top_words_per_year = {}\n",
    "    for year in range(2010, 2024):  # Adjust range to include 2023\n",
    "        start_month = (year - 2010) * 12\n",
    "        end_month = start_month + 12\n",
    "        if year == 2023:  # Adjust for 2023 having data only up to October\n",
    "            end_month = start_month + 10\n",
    "        word_freqs = {}\n",
    "        for month_index in range(start_month, end_month):\n",
    "            if month_index < len(model.time_slices):\n",
    "                topic_dist = model.show_topic(topic_id, time=month_index, num_words=num_words)\n",
    "                for prob, word in topic_dist:\n",
    "                    if word in exclude_words:\n",
    "                        continue\n",
    "                    if word not in word_freqs:\n",
    "                        word_freqs[word] = 0\n",
    "                    word_freqs[word] += prob\n",
    "        # Sort and select the top words\n",
    "        sorted_words = sorted(word_freqs.items(), key=lambda x: x[1], reverse=True)[:num_words]\n",
    "        top_words_per_year[year] = [word for word, _ in sorted_words]\n",
    "    top_words_per_topic_year[topic_names[topic_id]] = top_words_per_year\n",
    "\n",
    "# Convert the results to a DataFrame for easy viewing and saving\n",
    "df_top_words = pd.DataFrame.from_dict({(i, j): top_words_per_topic_year[i][j] \n",
    "                           for i in top_words_per_topic_year.keys() \n",
    "                           for j in top_words_per_topic_year[i].keys()},\n",
    "                       orient='index')\n",
    "\n",
    "df_top_words.index = pd.MultiIndex.from_tuples(df_top_words.index, names=['Topic', 'Year'])\n",
    "df_top_words = df_top_words.reset_index()\n",
    "\n",
    "print(\"Top 10 Words per Topic per Year:\")\n",
    "print(df_top_words)\n",
    "\n",
    "# Save results to CSV\n",
    "df_top_words.to_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Text-Tables/top_words_per_topic_per_year.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909d4a00",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Climate/Transition_Politics_&_Finance.tex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Save LaTeX code to files\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic, latex_code \u001b[38;5;129;01min\u001b[39;00m latex_tables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     55\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(latex_code)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Output LaTeX code for one of the topics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Climate/Transition_Politics_&_Finance.tex'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the CSV file (assuming df_top_words is already created)\n",
    "df_top_words = pd.read_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Text-Tables/top_words_per_topic_per_year.csv')\n",
    "\n",
    "# Function to filter the DataFrame for a specific topic and get top 15 words per year\n",
    "def get_topic_words(df, topic):\n",
    "    topic_df = df[df['Topic'] == topic]\n",
    "    topic_words = {}\n",
    "    for year in topic_df['Year'].unique():\n",
    "        words = topic_df[topic_df['Year'] == year].iloc[:, 2:17].values.flatten().tolist()  # Select top 15 words\n",
    "        topic_words[year] = words\n",
    "    return topic_words\n",
    "\n",
    "# Get the top 15 words per year for the specified topics\n",
    "topics = ['Climate Change Impact', 'Energy Policies & Renewables', 'Climate/Transition Politics & Finance']\n",
    "topic_words_data = {topic: get_topic_words(df_top_words, topic) for topic in topics}\n",
    "\n",
    "# Function to generate LaTeX table for a topic\n",
    "def generate_latex_table(topic, words_data):\n",
    "    years = sorted(words_data.keys())\n",
    "    words = {year: words_data[year][:15] for year in years}\n",
    "    \n",
    "    # Extract unique words and their years\n",
    "    unique_words = set(word for year in years for word in words[year])\n",
    "    word_years = {word: [year for year in years if word in words[year]] for word in unique_words}\n",
    "    \n",
    "    # Generate LaTeX code\n",
    "    latex_code = f\"\\\\begin{{table}}[htbp]\\n\\\\centering\\n\\\\begin{{tabular}}{{|l|\" + \"c|\" * len(years) + \"}}\\n\"\n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    latex_code += \" & \" + \" & \".join(map(str, years)) + \" \\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    \n",
    "    for word in sorted(unique_words):\n",
    "        latex_code += word\n",
    "        for year in years:\n",
    "            if word in words[year]:\n",
    "                color = f\"\\\\textcolor{{red}}{{{word}}}\"  # Example: color in red, change as needed\n",
    "                latex_code += f\" & {color}\"\n",
    "            else:\n",
    "                latex_code += \" & \"\n",
    "        latex_code += \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    latex_code += \"\\\\end{tabular}\\n\\\\caption{Top 15 words for the topic: \" + topic + \"}\\n\\\\end{table}\\n\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Generate LaTeX tables for each topic\n",
    "latex_tables = {topic: generate_latex_table(topic, topic_words_data[topic]) for topic in topics}\n",
    "\n",
    "# Save LaTeX code to files\n",
    "for topic, latex_code in latex_tables.items():\n",
    "    with open(f\"{topic.replace(' ', '_')}.tex\", 'w') as file:\n",
    "        file.write(latex_code)\n",
    "\n",
    "# Output LaTeX code for one of the topics\n",
    "print(latex_tables['Climate Change Impact'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c41f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}}\n",
      "\\hline\n",
      " & 2010 & 2011 & 2012 & 2013 & 2014 & 2015 & 2016 & 2017 & 2018 & 2019 & 2020 & 2021 & 2022 & 2023 \\\\\n",
      "\\hline\n",
      "cape & \\textcolor{red}{cape} & \\textcolor{red}{cape} &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "car & \\textcolor{red}{car} & \\textcolor{red}{car} & \\textcolor{red}{car} & \\textcolor{red}{car} &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "chang & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} & \\textcolor{red}{chang} \\\\\n",
      "citi & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} & \\textcolor{red}{citi} &  &  & \\textcolor{red}{citi} \\\\\n",
      "clean &  &  &  &  &  &  & \\textcolor{red}{clean} &  &  &  &  &  &  &  \\\\\n",
      "climat & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} & \\textcolor{red}{climat} \\\\\n",
      "commun &  &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{commun} &  \\\\\n",
      "countri & \\textcolor{red}{countri} &  &  &  &  &  &  &  &  &  & \\textcolor{red}{countri} & \\textcolor{red}{countri} &  &  \\\\\n",
      "drought & \\textcolor{red}{drought} &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{drought} &  &  \\\\\n",
      "durban &  &  &  & \\textcolor{red}{durban} &  &  & \\textcolor{red}{durban} & \\textcolor{red}{durban} & \\textcolor{red}{durban} & \\textcolor{red}{durban} &  &  &  &  \\\\\n",
      "electr & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} & \\textcolor{red}{electr} &  &  &  &  &  &  \\\\\n",
      "environ &  &  &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{environ} \\\\\n",
      "environment &  &  &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{environment} \\\\\n",
      "flood &  & \\textcolor{red}{flood} &  &  & \\textcolor{red}{flood} & \\textcolor{red}{flood} &  &  &  &  &  &  & \\textcolor{red}{flood} &  \\\\\n",
      "food & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} & \\textcolor{red}{food} \\\\\n",
      "fuel &  &  &  &  &  &  &  &  &  & \\textcolor{red}{fuel} &  &  &  &  \\\\\n",
      "futur &  &  &  &  &  & \\textcolor{red}{futur} &  &  &  &  &  &  & \\textcolor{red}{futur} &  \\\\\n",
      "global &  &  &  & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} & \\textcolor{red}{global} \\\\\n",
      "human &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{human} & \\textcolor{red}{human} & \\textcolor{red}{human} \\\\\n",
      "increas &  &  &  &  &  &  &  &  &  & \\textcolor{red}{increas} & \\textcolor{red}{increas} & \\textcolor{red}{increas} &  &  \\\\\n",
      "joburg &  &  &  &  & \\textcolor{red}{joburg} &  &  &  &  &  &  &  &  &  \\\\\n",
      "mean &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{mean} & \\textcolor{red}{mean} &  &  \\\\\n",
      "natur &  &  & \\textcolor{red}{natur} &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "need &  & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} & \\textcolor{red}{need} \\\\\n",
      "ocean &  &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{ocean} & \\textcolor{red}{ocean} \\\\\n",
      "peopl & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} & \\textcolor{red}{peopl} \\\\\n",
      "rise &  &  &  &  &  &  &  &  & \\textcolor{red}{rise} &  &  &  &  &  \\\\\n",
      "sea &  &  &  &  &  &  &  & \\textcolor{red}{sea} & \\textcolor{red}{sea} &  &  &  &  &  \\\\\n",
      "system & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} & \\textcolor{red}{system} &  & \\textcolor{red}{system} & \\textcolor{red}{system} \\\\\n",
      "temperatur &  &  &  &  &  &  &  &  & \\textcolor{red}{temperatur} & \\textcolor{red}{temperatur} & \\textcolor{red}{temperatur} &  &  &  \\\\\n",
      "town & \\textcolor{red}{town} & \\textcolor{red}{town} &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "us &  &  & \\textcolor{red}{us} & \\textcolor{red}{us} & \\textcolor{red}{us} & \\textcolor{red}{us} & \\textcolor{red}{us} & \\textcolor{red}{us} &  &  &  &  &  &  \\\\\n",
      "vehicl & \\textcolor{red}{vehicl} & \\textcolor{red}{vehicl} & \\textcolor{red}{vehicl} &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "warm &  &  &  &  &  &  &  &  &  &  &  & \\textcolor{red}{warm} &  &  \\\\\n",
      "wast &  &  & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} & \\textcolor{red}{wast} \\\\\n",
      "water & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} & \\textcolor{red}{water} \\\\\n",
      "world & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} & \\textcolor{red}{world} \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Top 15 words for the topic: Climate Change Impact}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_top_words is already loaded and formatted as described\n",
    "# It should have columns: 'Topic', 'Year', and 15 columns for the top words\n",
    "\n",
    "# Sample structure of df_top_words (replace with actual loading code if necessary)\n",
    "# df_top_words = pd.read_csv('path_to_your_top_words_per_topic_per_year.csv')\n",
    "\n",
    "# Convert the DataFrame into the required dictionary format\n",
    "def df_to_dict(df):\n",
    "    topics = df['Topic'].unique()\n",
    "    data = {}\n",
    "    for topic in topics:\n",
    "        topic_data = df[df['Topic'] == topic]\n",
    "        years = topic_data['Year'].unique()\n",
    "        data[topic] = {year: topic_data[topic_data['Year'] == year].iloc[0, 2:].tolist() for year in years}\n",
    "    return data\n",
    "\n",
    "# Load the actual DataFrame\n",
    "df_top_words = pd.read_csv('/Users/giacomoraederscheidt/Dropbox/Paper_Giacomo_Lotti/0 Text-Tables/top_words_per_topic_per_year.csv')\n",
    "\n",
    "# Process DataFrame to dictionary\n",
    "data = df_to_dict(df_top_words)\n",
    "\n",
    "# Function to generate LaTeX table for a topic\n",
    "def generate_latex_table(topic, data):\n",
    "    years = sorted(data[topic].keys())\n",
    "    words = {year: data[topic][year][:15] for year in years}\n",
    "    \n",
    "    # Extract unique words and their years\n",
    "    unique_words = set(word for year in years for word in words[year])\n",
    "    word_years = {word: [year for year in years if word in words[year]] for word in unique_words}\n",
    "    \n",
    "    # Generate LaTeX code\n",
    "    latex_code = f\"\\\\begin{{table}}[htbp]\\n\\\\centering\\n\\\\begin{{tabular}}{{|l|\" + \"c|\" * len(years) + \"}}\\n\"\n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    latex_code += \" & \" + \" & \".join(map(str, years)) + \" \\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    \n",
    "    for word in sorted(unique_words):\n",
    "        latex_code += word\n",
    "        for year in years:\n",
    "            if word in words[year]:\n",
    "                color = f\"\\\\textcolor{{red}}{{{word}}}\"  # Example: color in red, change as needed\n",
    "                latex_code += f\" & {color}\"\n",
    "            else:\n",
    "                latex_code += \" & \"\n",
    "        latex_code += \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_code += \"\\\\hline\\n\"\n",
    "    latex_code += \"\\\\end{tabular}\\n\\\\caption{Top 15 words for the topic: \" + topic + \"}\\n\\\\end{table}\\n\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Generate LaTeX tables for each topic\n",
    "topics = ['Climate Change Impact', 'Energy Policies & Renewables', 'Climate/Transition Politics & Finance']\n",
    "latex_tables = {topic: generate_latex_table(topic, data) for topic in topics}\n",
    "\n",
    "# Save LaTeX code to files\n",
    "for topic, latex_code in latex_tables.items():\n",
    "    safe_topic = topic.replace(' ', '_').replace('/', '_').replace('&', 'and')\n",
    "    with open(f\"{safe_topic}.tex\", 'w') as file:\n",
    "        file.write(latex_code)\n",
    "\n",
    "# Output LaTeX code for one of the topics\n",
    "print(latex_tables['Climate Change Impact'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169863d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
